# -*- coding: utf-8 -*-
"""collaborative_filtering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rf1I9o8nAFk6TJnWayeBaliSm-aaiTIY
"""

from numpy import array
import random
import time
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import sys
# print(sys.path)

df = pd.read_csv(r"./dice_com-job_us_sample.csv")
df1 = pd.read_csv(r"./survey_results_public.csv")
print(df.head())
l = list(df["companyName"])
# print(type(l))

companies = set(l)
# print(len(companies))  # Total number of companies 4292.
# print(companies)
companies = list(companies)
len(companies)
# print(companies[100])  # CompanyName at 100th position.

# Assign random companies to each respondent
cbf = pd.DataFrame(df1['Respondent'])
start = time.time()
for i in range(98855):
    r = random.randint(1, 4291)
    cbf.loc[i, "companyName"] = l[r]
end = time.time()
# print(end-start)
# print(cbf.head())

cbf.to_csv(r"./colabdata.csv")

# merge all the dataset having skills from user profile.
dflang = pd.read_csv(r"./LanguageWorkedWith.csv")
dfdata = pd.read_csv(r"./DatabaseWorkedWith.csv")
dfplat = pd.read_csv(r"./PlatformWorkedWith.csv")
dfframe = pd.read_csv(r"./FrameworkWorkedWith.csv")
dfdev = pd.read_csv(r"./DevType.csv")
dfmerge = pd.merge(dflang, dfdata, on="Respondent")
dfmerge = pd.merge(dfmerge, dfplat, on="Respondent")
dfmerge = pd.merge(dfmerge, dfframe, on="Respondent")
dfmerge = pd.merge(dfmerge, dfdev, on="Respondent")
#dfmerge.to_csv("./data/collaborative filtering/userskills.csv")

dfuser = pd.read_csv("./userskills.csv")
# remove duplicate columns during merge
dfuser = dfuser.loc[:, ~dfuser.columns.duplicated()]
dfuser = dfuser.fillna(0)
dfuser.shape
dfuser.head()
# remove rows with less than 5 skills
dfuser = dfuser.dropna(thresh=5)
dfuser.shape
m = (np.asscalar(np.int32(max(dfuser["Respondent"]))))
# print(m)
# print(type(m))
# Build  a dictionary of respondent id's as keys  and thier skills as values
temp = [0]*m
vector = np.array(temp)
count = 1
star = time.time()
d = dict()
for row in dfuser.iterrows():
    index, data = row
    l = list()
    # l=[data.values[0],list(data.values[1:])]
    s = np.asscalar(np.int32(data.values[0]))
    d[s] = np.array(list(data.values[1:]))
    # print(type(data))
end = time.time()
# print(d[1])
# print(end-star)

# Build the user user similarity matrix based on thier skill vectors
# initialise a user user matrix uptill respondent number 5000
sim = list()
s = time.time()
for i in range(5000):
    l = list()
    l = [0]*5000
    sim.append(l)
e = time.time()
# print(e-s)

count1 = 1
count2 = 1
t = time.time()
for key, value in d.items():
    if(key < 5000):
        # print(key)
        b = (np.linalg.norm(value))
        for key2, value2 in d.items():
            if(key2 < 5000):
                # print(key2)
                a = np.dot(d[key], d[key2])
                ans = a/(np.linalg.norm(value2)*b)
                sim[key][key2] = ans
                # count2+=1
        # count1+=1
e = time.time()
# print("total time for building similarity matrix ")

# print(e-s)
# print(sim[1][:10])
# the similarity of user 1 with others
# print((max(sim[1][2:])))
# print(sim[1].index(max(sim[1][2:])))
# user 1 is most similar to user 4653 with similarity 0.71 in skills

# Lets check with the random jobs given to the users .
dfjob = pd.read_csv("./colabdata.csv")


def collaborative_filtering(candidate):
    
    global sim
    # Recommend user 3 a job based on another user who has almost the same skill as him.
    print("Respondent {} is working in ".format(candidate))
    print(dfjob.loc[dfjob.Respondent == candidate]["companyName"].values)

    m1 = max(sim[candidate][:3])
    m2 = max(sim[candidate][4:])
    ma = max(m1, m2)
    suser = sim[candidate].index(ma)
    # print(suser) #user 3265 is very similar to user 3 and hence we can recommend user 3265 job to user 3.
    print("Respondent ", suser, "is working in")
    print(dfjob.loc[dfjob.Respondent == suser]["companyName"].values)

    # For buiding  collaborative filtering based on The Content based recommendations for the first 200 Respondents.
    dfcont = pd.read_csv("./recommendations.csv")
    sim = list()
    s = time.time()
    for i in range(200):
        l = list()
        l = [0]*200
        sim.append(l)
    e = time.time()
    # print(e-s)
    t = time.time()
    print("Building the similarity matrix....\n")
    for key, value in d.items():
        if(key < 200):
            # print(key)
            b = (np.linalg.norm(value))
            for key2, value2 in d.items():
                if(key2 < 200):
                    # print(key2)
                    a = np.dot(d[key], d[key2])
                    ans = a/(np.linalg.norm(value2)*b)
                    sim[key][key2] = ans
                    # count2+=1
            # count1+=1
    e = time.time()

    # Recommend user 3 a job based on another user who has almost the same skill as him.
    print("Respondent 3 was recommended jobs from content based filtering in ")
    print(dfcont.loc[dfcont.Respondent == candidate]["companyName"].values)
    print("Respondent 3 was recommended job titles from content based filtering ")
    print(dfcont.loc[dfcont.Respondent == candidate]["jobTitle"].values)
    print("\n")
    m1 = max(sim[candidate][:3])
    m2 = max(sim[candidate][4:])
    ma = max(m1, m2)
    suser = sim[candidate].index(ma)
    # print(suser) #user 3265 is very similar to user 3 and hence we can recommend user 3265 job to user 3.
    print("Respondent ", suser, "was most similiar to respondent 3\n")
    print("Based on respondent ", suser, " the jobs recommended to 3 are ")
    print(dfcont.loc[dfcont.Respondent == suser]["companyName"].values)
    print("The recommended job titles are ")
    print(dfcont.loc[dfcont.Respondent == suser]["jobTitle"].values)
    return {"companies": dfcont.loc[dfcont.Respondent == suser]["companyName"].values.tolist(), "titles": dfcont.loc[dfcont.Respondent == suser]["jobTitle"].values.tolist()}
